{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Based MT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPbVnUEL5yqsV4MHV7lcHgp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karumugamio/NLAProjEnglishtoSimpleEnglishMT/blob/master/Transformer_Based_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMUJHjxLeE2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEaMzAxFeODI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "9176be13-786b-4531-d9fe-79a7fe9f1ec9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "import os\n",
        "os.chdir('/gdrive/My Drive/NLAProjectWS')\n",
        "os.listdir()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data',\n",
              " 'workspace',\n",
              " 'Readme.md',\n",
              " 'merged_dataset.txt',\n",
              " 'X_train.pkl',\n",
              " 'X_test.pkl',\n",
              " 'training_checkpoints',\n",
              " 'EDA',\n",
              " '.ipynb_checkpoints',\n",
              " 'merged_PD_v1Data.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiAlg-8Gfx-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "input_simple = '/gdrive/My Drive/NLAProjectWS/Data/v1_wiki.simple'\n",
        "input_en = '/gdrive/My Drive/NLAProjectWS/Data/v1_wiki.unsimplified'\n",
        "\n",
        "input_simple = '/gdrive/My Drive/NLAProjectWS/Data/v1_simple.training.txt'\n",
        "input_en = '/gdrive/My Drive/NLAProjectWS/Data/v1_normal.training.txt'\n",
        "\n",
        "\n",
        "\n",
        "input_simple = '/gdrive/My Drive/NLAProjectWS/Data/v1_simple.tuning.txt'\n",
        "input_en = '/gdrive/My Drive/NLAProjectWS/Data/v1_normal.tuning.txt'\n",
        "\n",
        "\n",
        "en_dataset=pd.read_csv(input_en,delimiter=\"\\n\",header=None,names = ['enSrc'])\n",
        "simple_dataset = pd.read_csv(input_simple,delimiter=\"\\n\",header = None,names = ['simpleSrc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S98-mYmMgfp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtsKkvAff451",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_data_en = en_dataset['enSrc'].tolist()\n",
        "raw_data_simple = simple_dataset['simpleSrc'].tolist()\n",
        "raw_data_en, raw_data_simple = list(raw_data_en), list(raw_data_simple)\n",
        "raw_data_en = [normalize_string(data) for data in raw_data_en]\n",
        "raw_data_si_in = ['<start> ' + normalize_string(data) for data in raw_data_simple]\n",
        "raw_data_si_out = [normalize_string(data) + ' <end>' for data in raw_data_simple]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMpyqTXxhCOF",
        "colab_type": "text"
      },
      "source": [
        "Tokenizing all Src Contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8vMSjP8glry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n",
        "                                                        padding='post')\n",
        "\n",
        "simple_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "simple_tokenizer.fit_on_texts(raw_data_si_in)\n",
        "simple_tokenizer.fit_on_texts(raw_data_si_out)\n",
        "data_si_in = simple_tokenizer.texts_to_sequences(raw_data_si_in)\n",
        "data_si_in = tf.keras.preprocessing.sequence.pad_sequences(data_si_in,\n",
        "                                                           padding='post')\n",
        "\n",
        "data_si_out = simple_tokenizer.texts_to_sequences(raw_data_si_out)\n",
        "data_si_out = tf.keras.preprocessing.sequence.pad_sequences(data_si_out,\n",
        "                                                            padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGmHsHPnhGgM",
        "colab_type": "text"
      },
      "source": [
        "Create tf.data.Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtvMozryhAcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_en, data_si_in, data_si_out))\n",
        "dataset = dataset.shuffle(20).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ41ovI1hOZM",
        "colab_type": "text"
      },
      "source": [
        "Create the Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dgxPwPOhQak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_embedding(pos, model_size):\n",
        "    PE = np.zeros((1, model_size))\n",
        "    for i in range(model_size):\n",
        "        if i % 2 == 0:\n",
        "            PE[:, i] = np.sin(pos / 10000 ** (i / model_size))\n",
        "        else:\n",
        "            PE[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
        "    return PE\n",
        "\n",
        "max_length = max(len(data_en[0]), len(data_si_in[0]))\n",
        "MODEL_SIZE = 128\n",
        "\n",
        "pes = []\n",
        "for i in range(max_length):\n",
        "    pes.append(positional_embedding(i, MODEL_SIZE))\n",
        "\n",
        "pes = np.concatenate(pes, axis=0)\n",
        "pes = tf.constant(pes, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxkj-tMDhu_1",
        "colab_type": "text"
      },
      "source": [
        "Create the Multihead Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-CMbCAOhqgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.Model):\n",
        "    def __init__(self, model_size, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.query_size = model_size // h\n",
        "        self.key_size = model_size // h\n",
        "        self.value_size = model_size // h\n",
        "        self.h = h\n",
        "        self.wq = [tf.keras.layers.Dense(self.query_size) for _ in range(h)]\n",
        "        self.wk = [tf.keras.layers.Dense(self.key_size) for _ in range(h)]\n",
        "        self.wv = [tf.keras.layers.Dense(self.value_size) for _ in range(h)]\n",
        "        self.wo = tf.keras.layers.Dense(model_size)\n",
        "\n",
        "    def call(self, decoder_output, encoder_output):\n",
        "        # decoder_output has shape (batch, decoder_len, model_size)\n",
        "        # encoder_output has shape (batch, encoder_len, model_size)\n",
        "        heads = []\n",
        "        for i in range(self.h):\n",
        "            score = tf.matmul(self.wq[i](decoder_output), self.wk[i](encoder_output), transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, tf.float32))\n",
        "            # score has shape (batch, decoder_len, encoder_len)\n",
        "            alignment = tf.nn.softmax(score, axis=2)\n",
        "            # alignment has shape (batch, decoder_len, encoder_len)\n",
        "            head = tf.matmul(alignment, self.wv[i](encoder_output))\n",
        "            # head has shape (batch, decoder_len, value_size)\n",
        "            heads.append(head)\n",
        "        heads = tf.concat(heads, axis=2)\n",
        "        heads = self.wo(heads)\n",
        "        # heads has shape (batch, decoder_len, model_size)\n",
        "        return heads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to4k9XjahyDE",
        "colab_type": "text"
      },
      "source": [
        "## Create the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Y6KW74h0-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.attention = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        \n",
        "        self.attention_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        \n",
        "        self.dense_1 = [tf.keras.layers.Dense(512, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(model_size) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        \n",
        "    def call(self, sequence):\n",
        "        sub_in = []\n",
        "        for i in range(sequence.shape[1]):\n",
        "            embed = self.embedding(tf.expand_dims(sequence[:, i], axis=1))\n",
        "            sub_in.append(embed + pes[i, :])\n",
        "            \n",
        "        sub_in = tf.concat(sub_in, axis=1)\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            sub_out = []\n",
        "            for j in range(sub_in.shape[1]):\n",
        "                attention = self.attention[i](\n",
        "                    tf.expand_dims(sub_in[:, j, :], axis=1), sub_in)\n",
        "\n",
        "                sub_out.append(attention)\n",
        "\n",
        "            sub_out = tf.concat(sub_out, axis=1)\n",
        "            sub_out = sub_in + sub_out\n",
        "            sub_out = self.attention_norm[i](sub_out)\n",
        "            \n",
        "            ffn_in = sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = ffn_in + ffn_out\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            sub_in = ffn_out\n",
        "            \n",
        "        return ffn_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivfs1LTvh4e8",
        "colab_type": "text"
      },
      "source": [
        "## Creating Decoders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB4JKiVvh7Ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.attention_bot = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_bot_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        self.attention_mid = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_mid_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        \n",
        "        self.dense_1 = [tf.keras.layers.Dense(512, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(model_size) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        \n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, sequence, encoder_output):\n",
        "        # EMBEDDING AND POSITIONAL EMBEDDING\n",
        "        embed_out = []\n",
        "        for i in range(sequence.shape[1]):\n",
        "            embed = self.embedding(tf.expand_dims(sequence[:, i], axis=1))\n",
        "            embed_out.append(embed + pes[i, :])\n",
        "            \n",
        "        embed_out = tf.concat(embed_out, axis=1)\n",
        "        \n",
        "        \n",
        "        bot_sub_in = embed_out\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            # BOTTOM MULTIHEAD SUB LAYER\n",
        "            bot_sub_out = []\n",
        "            \n",
        "            for j in range(bot_sub_in.shape[1]):\n",
        "                values = bot_sub_in[:, :j, :]\n",
        "                attention = self.attention_bot[i](\n",
        "                    tf.expand_dims(bot_sub_in[:, j, :], axis=1), values)\n",
        "\n",
        "                bot_sub_out.append(attention)\n",
        "            bot_sub_out = tf.concat(bot_sub_out, axis=1)\n",
        "            bot_sub_out = bot_sub_in + bot_sub_out\n",
        "            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n",
        "            \n",
        "            # MIDDLE MULTIHEAD SUB LAYER\n",
        "            mid_sub_in = bot_sub_out\n",
        "\n",
        "            mid_sub_out = []\n",
        "            for j in range(mid_sub_in.shape[1]):\n",
        "                attention = self.attention_mid[i](\n",
        "                    tf.expand_dims(mid_sub_in[:, j, :], axis=1), encoder_output)\n",
        "\n",
        "                mid_sub_out.append(attention)\n",
        "\n",
        "            mid_sub_out = tf.concat(mid_sub_out, axis=1)\n",
        "            mid_sub_out = mid_sub_out + mid_sub_in\n",
        "            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n",
        "\n",
        "            # FFN\n",
        "            ffn_in = mid_sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = ffn_out + ffn_in\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            bot_sub_in = ffn_out\n",
        "        \n",
        "        logits = self.dense(ffn_out)\n",
        "            \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK3mk60UiKpG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "abec65cd-a2d1-431b-9ba9-6f26210a0115"
      },
      "source": [
        "H = 2\n",
        "NUM_LAYERS = 2\n",
        "\n",
        "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "encoder = Encoder(en_vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "\n",
        "en_sequence_in = tf.constant([[1, 2, 3, 4, 6, 7, 8, 0, 0, 0], \n",
        "                           [1, 2, 3, 4, 6, 7, 8, 0, 0, 0]])\n",
        "encoder_output = encoder(en_sequence_in)\n",
        "\n",
        "print('Input vocabulary size', en_vocab_size)\n",
        "print('Encoder input shape', en_sequence_in.shape)\n",
        "print('Encoder output shape', encoder_output.shape)\n",
        "\n",
        "si_vocab_size = len(simple_tokenizer.word_index) + 1\n",
        "max_len_simple = data_si_in.shape[1]\n",
        "decoder = Decoder(si_vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "\n",
        "si_sequence_in = tf.constant([[1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0],\n",
        "                           [1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0]])\n",
        "decoder_output = decoder(si_sequence_in, encoder_output)\n",
        "\n",
        "print('Target vocabulary size', si_vocab_size)\n",
        "print('Decoder input shape', si_sequence_in.shape)\n",
        "print('Decoder output shape', decoder_output.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input vocabulary size 28560\n",
            "Encoder input shape (2, 10)\n",
            "Encoder output shape (2, 10, 128)\n",
            "Target vocabulary size 25726\n",
            "Decoder input shape (2, 14)\n",
            "Decoder output shape (2, 14, 25726)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJOAVpVFiq4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True)\n",
        "def loss_func(targets, logits):\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRhXO-95it40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_source_text=None):\n",
        "    if test_source_text is None:\n",
        "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
        "    print(test_source_text)\n",
        "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
        "    print(test_source_seq)\n",
        "\n",
        "    en_output = encoder(tf.constant(test_source_seq))\n",
        "\n",
        "    de_input = tf.constant([[simple_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
        "\n",
        "    out_words = []\n",
        "\n",
        "    while True:\n",
        "        de_output = decoder(de_input, en_output)\n",
        "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
        "        out_words.append(simple_tokenizer.index_word[new_word.numpy()[0][0]])\n",
        "\n",
        "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 14:\n",
        "            break\n",
        "\n",
        "    print(' '.join(out_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBIjIKdljBGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(source_seq, target_seq_in, target_seq_out):\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_output = encoder(source_seq)\n",
        "        \n",
        "        decoder_output = decoder(target_seq_in, encoder_output)\n",
        "\n",
        "        loss = loss_func(target_seq_out, decoder_output)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79IjVKj5jDTn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29e0bf62-70dc-4bab-ff5b-04da9e729e66"
      },
      "source": [
        "NUM_EPOCHS = 2\n",
        "batch_counter = 0\n",
        "start_time = time.time()\n",
        "for e in range(NUM_EPOCHS):\n",
        "    print(\"Epoch Started: {}\".format(e+1))\n",
        "    batch_counter = 0\n",
        "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "      batch_counter = batch_counter + 1\n",
        "      print('Epoch {} Batch Count {:.4f}'.format(e + 1, batch_counter)) \n",
        "      loss = train_step(source_seq, target_seq_in,target_seq_out)\n",
        "       \n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(e + 1, loss.numpy()))\n",
        "\n",
        "    if (e + 1) % 10 == 0:\n",
        "        end_time = time.time()\n",
        "        print('Average elapsed time: {:.2f}s'.format((end_time - start_time) / (e + 1)))\n",
        "        try:\n",
        "            predict()\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Started: 1\n",
            "Epoch 1 Batch Count 1.0000\n",
            "Epoch 1 Batch Count 2.0000\n",
            "Epoch 1 Batch Count 3.0000\n",
            "Epoch 1 Batch Count 4.0000\n",
            "Epoch 1 Batch Count 5.0000\n",
            "Epoch 1 Batch Count 6.0000\n",
            "Epoch 1 Batch Count 7.0000\n",
            "Epoch 1 Batch Count 8.0000\n",
            "Epoch 1 Batch Count 9.0000\n",
            "Epoch 1 Batch Count 10.0000\n",
            "Epoch 1 Batch Count 11.0000\n",
            "Epoch 1 Batch Count 12.0000\n",
            "Epoch 1 Batch Count 13.0000\n",
            "Epoch 1 Batch Count 14.0000\n",
            "Epoch 1 Batch Count 15.0000\n",
            "Epoch 1 Batch Count 16.0000\n",
            "Epoch 1 Batch Count 17.0000\n",
            "Epoch 1 Batch Count 18.0000\n",
            "Epoch 1 Batch Count 19.0000\n",
            "Epoch 1 Batch Count 20.0000\n",
            "Epoch 1 Batch Count 21.0000\n",
            "Epoch 1 Batch Count 22.0000\n",
            "Epoch 1 Batch Count 23.0000\n",
            "Epoch 1 Batch Count 24.0000\n",
            "Epoch 1 Batch Count 25.0000\n",
            "Epoch 1 Batch Count 26.0000\n",
            "Epoch 1 Batch Count 27.0000\n",
            "Epoch 1 Batch Count 28.0000\n",
            "Epoch 1 Batch Count 29.0000\n",
            "Epoch 1 Batch Count 30.0000\n",
            "Epoch 1 Batch Count 31.0000\n",
            "Epoch 1 Batch Count 32.0000\n",
            "Epoch 1 Batch Count 33.0000\n",
            "Epoch 1 Batch Count 34.0000\n",
            "Epoch 1 Batch Count 35.0000\n",
            "Epoch 1 Batch Count 36.0000\n",
            "Epoch 1 Batch Count 37.0000\n",
            "Epoch 1 Batch Count 38.0000\n",
            "Epoch 1 Batch Count 39.0000\n",
            "Epoch 1 Batch Count 40.0000\n",
            "Epoch 1 Batch Count 41.0000\n",
            "Epoch 1 Batch Count 42.0000\n",
            "Epoch 1 Batch Count 43.0000\n",
            "Epoch 1 Batch Count 44.0000\n",
            "Epoch 1 Batch Count 45.0000\n",
            "Epoch 1 Batch Count 46.0000\n",
            "Epoch 1 Batch Count 47.0000\n",
            "Epoch 1 Batch Count 48.0000\n",
            "Epoch 1 Batch Count 49.0000\n",
            "Epoch 1 Batch Count 50.0000\n",
            "Epoch 1 Batch Count 51.0000\n",
            "Epoch 1 Batch Count 52.0000\n",
            "Epoch 1 Batch Count 53.0000\n",
            "Epoch 1 Batch Count 54.0000\n",
            "Epoch 1 Batch Count 55.0000\n",
            "Epoch 1 Batch Count 56.0000\n",
            "Epoch 1 Batch Count 57.0000\n",
            "Epoch 1 Batch Count 58.0000\n",
            "Epoch 1 Batch Count 59.0000\n",
            "Epoch 1 Batch Count 60.0000\n",
            "Epoch 1 Batch Count 61.0000\n",
            "Epoch 1 Batch Count 62.0000\n",
            "Epoch 1 Batch Count 63.0000\n",
            "Epoch 1 Batch Count 64.0000\n",
            "Epoch 1 Batch Count 65.0000\n",
            "Epoch 1 Batch Count 66.0000\n",
            "Epoch 1 Batch Count 67.0000\n",
            "Epoch 1 Batch Count 68.0000\n",
            "Epoch 1 Batch Count 69.0000\n",
            "Epoch 1 Batch Count 70.0000\n",
            "Epoch 1 Batch Count 71.0000\n",
            "Epoch 1 Batch Count 72.0000\n",
            "Epoch 1 Batch Count 73.0000\n",
            "Epoch 1 Batch Count 74.0000\n",
            "Epoch 1 Batch Count 75.0000\n",
            "Epoch 1 Batch Count 76.0000\n",
            "Epoch 1 Batch Count 77.0000\n",
            "Epoch 1 Batch Count 78.0000\n",
            "Epoch 1 Batch Count 79.0000\n",
            "Epoch 1 Batch Count 80.0000\n",
            "Epoch 1 Batch Count 81.0000\n",
            "Epoch 1 Batch Count 82.0000\n",
            "Epoch 1 Batch Count 83.0000\n",
            "Epoch 1 Batch Count 84.0000\n",
            "Epoch 1 Batch Count 85.0000\n",
            "Epoch 1 Batch Count 86.0000\n",
            "Epoch 1 Batch Count 87.0000\n",
            "Epoch 1 Batch Count 88.0000\n",
            "Epoch 1 Batch Count 89.0000\n",
            "Epoch 1 Batch Count 90.0000\n",
            "Epoch 1 Batch Count 91.0000\n",
            "Epoch 1 Batch Count 92.0000\n",
            "Epoch 1 Batch Count 93.0000\n",
            "Epoch 1 Batch Count 94.0000\n",
            "Epoch 1 Batch Count 95.0000\n",
            "Epoch 1 Batch Count 96.0000\n",
            "Epoch 1 Batch Count 97.0000\n",
            "Epoch 1 Batch Count 98.0000\n",
            "Epoch 1 Batch Count 99.0000\n",
            "Epoch 1 Batch Count 100.0000\n",
            "Epoch 1 Batch Count 101.0000\n",
            "Epoch 1 Batch Count 102.0000\n",
            "Epoch 1 Batch Count 103.0000\n",
            "Epoch 1 Batch Count 104.0000\n",
            "Epoch 1 Batch Count 105.0000\n",
            "Epoch 1 Batch Count 106.0000\n",
            "Epoch 1 Batch Count 107.0000\n",
            "Epoch 1 Batch Count 108.0000\n",
            "Epoch 1 Batch Count 109.0000\n",
            "Epoch 1 Batch Count 110.0000\n",
            "Epoch 1 Batch Count 111.0000\n",
            "Epoch 1 Batch Count 112.0000\n",
            "Epoch 1 Batch Count 113.0000\n",
            "Epoch 1 Batch Count 114.0000\n",
            "Epoch 1 Batch Count 115.0000\n",
            "Epoch 1 Batch Count 116.0000\n",
            "Epoch 1 Batch Count 117.0000\n",
            "Epoch 1 Batch Count 118.0000\n",
            "Epoch 1 Batch Count 119.0000\n",
            "Epoch 1 Batch Count 120.0000\n",
            "Epoch 1 Batch Count 121.0000\n",
            "Epoch 1 Batch Count 122.0000\n",
            "Epoch 1 Batch Count 123.0000\n",
            "Epoch 1 Batch Count 124.0000\n",
            "Epoch 1 Loss 0.9552\n",
            "Epoch Started: 2\n",
            "Epoch 2 Batch Count 1.0000\n",
            "Epoch 2 Batch Count 2.0000\n",
            "Epoch 2 Batch Count 3.0000\n",
            "Epoch 2 Batch Count 4.0000\n",
            "Epoch 2 Batch Count 5.0000\n",
            "Epoch 2 Batch Count 6.0000\n",
            "Epoch 2 Batch Count 7.0000\n",
            "Epoch 2 Batch Count 8.0000\n",
            "Epoch 2 Batch Count 9.0000\n",
            "Epoch 2 Batch Count 10.0000\n",
            "Epoch 2 Batch Count 11.0000\n",
            "Epoch 2 Batch Count 12.0000\n",
            "Epoch 2 Batch Count 13.0000\n",
            "Epoch 2 Batch Count 14.0000\n",
            "Epoch 2 Batch Count 15.0000\n",
            "Epoch 2 Batch Count 16.0000\n",
            "Epoch 2 Batch Count 17.0000\n",
            "Epoch 2 Batch Count 18.0000\n",
            "Epoch 2 Batch Count 19.0000\n",
            "Epoch 2 Batch Count 20.0000\n",
            "Epoch 2 Batch Count 21.0000\n",
            "Epoch 2 Batch Count 22.0000\n",
            "Epoch 2 Batch Count 23.0000\n",
            "Epoch 2 Batch Count 24.0000\n",
            "Epoch 2 Batch Count 25.0000\n",
            "Epoch 2 Batch Count 26.0000\n",
            "Epoch 2 Batch Count 27.0000\n",
            "Epoch 2 Batch Count 28.0000\n",
            "Epoch 2 Batch Count 29.0000\n",
            "Epoch 2 Batch Count 30.0000\n",
            "Epoch 2 Batch Count 31.0000\n",
            "Epoch 2 Batch Count 32.0000\n",
            "Epoch 2 Batch Count 33.0000\n",
            "Epoch 2 Batch Count 34.0000\n",
            "Epoch 2 Batch Count 35.0000\n",
            "Epoch 2 Batch Count 36.0000\n",
            "Epoch 2 Batch Count 37.0000\n",
            "Epoch 2 Batch Count 38.0000\n",
            "Epoch 2 Batch Count 39.0000\n",
            "Epoch 2 Batch Count 40.0000\n",
            "Epoch 2 Batch Count 41.0000\n",
            "Epoch 2 Batch Count 42.0000\n",
            "Epoch 2 Batch Count 43.0000\n",
            "Epoch 2 Batch Count 44.0000\n",
            "Epoch 2 Batch Count 45.0000\n",
            "Epoch 2 Batch Count 46.0000\n",
            "Epoch 2 Batch Count 47.0000\n",
            "Epoch 2 Batch Count 48.0000\n",
            "Epoch 2 Batch Count 49.0000\n",
            "Epoch 2 Batch Count 50.0000\n",
            "Epoch 2 Batch Count 51.0000\n",
            "Epoch 2 Batch Count 52.0000\n",
            "Epoch 2 Batch Count 53.0000\n",
            "Epoch 2 Batch Count 54.0000\n",
            "Epoch 2 Batch Count 55.0000\n",
            "Epoch 2 Batch Count 56.0000\n",
            "Epoch 2 Batch Count 57.0000\n",
            "Epoch 2 Batch Count 58.0000\n",
            "Epoch 2 Batch Count 59.0000\n",
            "Epoch 2 Batch Count 60.0000\n",
            "Epoch 2 Batch Count 61.0000\n",
            "Epoch 2 Batch Count 62.0000\n",
            "Epoch 2 Batch Count 63.0000\n",
            "Epoch 2 Batch Count 64.0000\n",
            "Epoch 2 Batch Count 65.0000\n",
            "Epoch 2 Batch Count 66.0000\n",
            "Epoch 2 Batch Count 67.0000\n",
            "Epoch 2 Batch Count 68.0000\n",
            "Epoch 2 Batch Count 69.0000\n",
            "Epoch 2 Batch Count 70.0000\n",
            "Epoch 2 Batch Count 71.0000\n",
            "Epoch 2 Batch Count 72.0000\n",
            "Epoch 2 Batch Count 73.0000\n",
            "Epoch 2 Batch Count 74.0000\n",
            "Epoch 2 Batch Count 75.0000\n",
            "Epoch 2 Batch Count 76.0000\n",
            "Epoch 2 Batch Count 77.0000\n",
            "Epoch 2 Batch Count 78.0000\n",
            "Epoch 2 Batch Count 79.0000\n",
            "Epoch 2 Batch Count 80.0000\n",
            "Epoch 2 Batch Count 81.0000\n",
            "Epoch 2 Batch Count 82.0000\n",
            "Epoch 2 Batch Count 83.0000\n",
            "Epoch 2 Batch Count 84.0000\n",
            "Epoch 2 Batch Count 85.0000\n",
            "Epoch 2 Batch Count 86.0000\n",
            "Epoch 2 Batch Count 87.0000\n",
            "Epoch 2 Batch Count 88.0000\n",
            "Epoch 2 Batch Count 89.0000\n",
            "Epoch 2 Batch Count 90.0000\n",
            "Epoch 2 Batch Count 91.0000\n",
            "Epoch 2 Batch Count 92.0000\n",
            "Epoch 2 Batch Count 93.0000\n",
            "Epoch 2 Batch Count 94.0000\n",
            "Epoch 2 Batch Count 95.0000\n",
            "Epoch 2 Batch Count 96.0000\n",
            "Epoch 2 Batch Count 97.0000\n",
            "Epoch 2 Batch Count 98.0000\n",
            "Epoch 2 Batch Count 99.0000\n",
            "Epoch 2 Batch Count 100.0000\n",
            "Epoch 2 Batch Count 101.0000\n",
            "Epoch 2 Batch Count 102.0000\n",
            "Epoch 2 Batch Count 103.0000\n",
            "Epoch 2 Batch Count 104.0000\n",
            "Epoch 2 Batch Count 105.0000\n",
            "Epoch 2 Batch Count 106.0000\n",
            "Epoch 2 Batch Count 107.0000\n",
            "Epoch 2 Batch Count 108.0000\n",
            "Epoch 2 Batch Count 109.0000\n",
            "Epoch 2 Batch Count 110.0000\n",
            "Epoch 2 Batch Count 111.0000\n",
            "Epoch 2 Batch Count 112.0000\n",
            "Epoch 2 Batch Count 113.0000\n",
            "Epoch 2 Batch Count 114.0000\n",
            "Epoch 2 Batch Count 115.0000\n",
            "Epoch 2 Batch Count 116.0000\n",
            "Epoch 2 Batch Count 117.0000\n",
            "Epoch 2 Batch Count 118.0000\n",
            "Epoch 2 Batch Count 119.0000\n",
            "Epoch 2 Batch Count 120.0000\n",
            "Epoch 2 Batch Count 121.0000\n",
            "Epoch 2 Batch Count 122.0000\n",
            "Epoch 2 Batch Count 123.0000\n",
            "Epoch 2 Batch Count 124.0000\n",
            "Epoch 2 Loss 0.8335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYPy-qVFk3nz",
        "colab_type": "text"
      },
      "source": [
        "This is End of This Model! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ynrmk5k6qs",
        "colab_type": "text"
      },
      "source": [
        "Add Step to Save the model and reload it "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv0XdCLenqtD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "875bedd3-6a99-4991-f0c3-4b837ce7c5f2"
      },
      "source": [
        "print('hello world')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FhBbL6Ixroa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_for_report(test_source_text=None):\n",
        "    if test_source_text is None:\n",
        "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
        "    print(test_source_text)\n",
        "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
        "    print(test_source_seq)\n",
        "\n",
        "    en_output = encoder(tf.constant(test_source_seq))\n",
        "\n",
        "    de_input = tf.constant([[simple_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
        "\n",
        "    out_words = []\n",
        "\n",
        "    while True:\n",
        "        de_output = decoder(de_input, en_output)\n",
        "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
        "        out_words.append(simple_tokenizer.index_word[new_word.numpy()[0][0]])\n",
        "\n",
        "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 14:\n",
        "            break\n",
        "\n",
        "    predicted_value = ' '.join(out_words)\n",
        "\n",
        "    return predicted_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_tBx5RBx1sf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5d249039-2e36-46e8-c9a0-d7685424f286"
      },
      "source": [
        "predict_for_report()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When his class arrives they discover that their substitute teacher has not shown up .\n",
            "[[57, 22, 367, 6822, 47, 6375, 17, 43, 10111, 1728, 27, 39, 1353, 100, 2]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'in is is the is the is and and and and and and and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abEwq7Zz7Ifx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_for_report2(test_source_text=None):\n",
        "    if test_source_text is None:\n",
        "        indexV= np.random.choice(len(raw_data_en))\n",
        "        test_source_text = raw_data_en[indexV]\n",
        "        actual_target = raw_data_simple[indexV]\n",
        "    #print(test_source_text)\n",
        "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
        "    #print(test_source_seq)\n",
        "\n",
        "    en_output = encoder(tf.constant(test_source_seq))\n",
        "\n",
        "    de_input = tf.constant([[simple_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
        "\n",
        "    out_words = []\n",
        "\n",
        "    while True:\n",
        "        de_output = decoder(de_input, en_output)\n",
        "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
        "        out_words.append(simple_tokenizer.index_word[new_word.numpy()[0][0]])\n",
        "\n",
        "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 14:\n",
        "            break\n",
        "\n",
        "    predicted_value = ' '.join(out_words)\n",
        "\n",
        "    return test_source_text,actual_target,predicted_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FpLcKpK7Ru7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c81042fd-baa4-4d0f-8daf-7763785c8b27"
      },
      "source": [
        "for i in range(30):\n",
        "  src,target,predicted = predict_for_report2()\n",
        "  print(\"Source Sentence {}\".format(src))\n",
        "  print(\"Target Sentence {}\".format(target))\n",
        "  print(\"Predicted Sentence {}\".format(predicted))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source Sentence The Dust Bowl or the Dirty Thirties was a period of severe dust storms causing major ecological and agricultural damage to American and Canadian prairie lands from to .\n",
            "Target Sentence The Dust Bowl was a period of severe dust storms causing major ecological and agricultural damage to American and Canadian and also the Chinese area prairie lands from 1930 to 1936 and also in some parts through the 1940s .\n",
            "Predicted Sentence in is is the first is the first is the is the is .\n",
            "Source Sentence Unguja comprises three administrative regions Zanzibar Central South Zanzibar North and Zanzibar Urban West .\n",
            "Target Sentence The capital of Zanzibar , located on the island of Unguja , is Zanzibar City .\n",
            "Predicted Sentence <end>\n",
            "Source Sentence The University of Adelaide The Flinders University of South Australia and The University of South Australia are the public universities .\n",
            "Target Sentence South Australia is one of the six states of Australia . Its Capital is Adelaide .\n",
            "Predicted Sentence of is is is the is the is of the is the is the\n",
            "Source Sentence Lithuanian is a Baltic language closely related to Latvian although they are not mutually intelligible .\n",
            "Target Sentence Lithuanian language along with Latvian are the only remained Baltic languages .\n",
            "Predicted Sentence of is is is is is is . <end>\n",
            "Source Sentence Thomas Allofs is a former German footballer who played as a striker .\n",
            "Target Sentence Thomas Allofs is a former German football player .\n",
            "Predicted Sentence <end>\n",
            "Source Sentence As of the population was and the newest population estimate is .\n",
            "Target Sentence As of 2000 , the population was 37,077 .\n",
            "Predicted Sentence of is is is the is of . <end>\n",
            "Source Sentence SummerSlam was a professional wrestling pay per view event produced by World Wrestling Entertainment which took place on August at Staples Center in Los Angeles California .\n",
            "Target Sentence SummerSlam was a professional wrestling pay-per-view event produced by World Wrestling Entertainment , which took place on August 23 , 2009 at Staples Center in Los Angeles , California .\n",
            "Predicted Sentence of is is is of . <end>\n",
            "Source Sentence Cao Cao then sent his trusted general Xiahou Dun to rescue Xiaopei but Xiahou Dun was defeated by Gao Shun . In the end Liu Bei had to desert the city to take shelter under Cao Cao .\n",
            "Target Sentence Xiahou Dun was an important general under Cao Cao .\n",
            "Predicted Sentence in was is is is of . <end>\n",
            "Source Sentence The Rubinstein Memorial tournament in his honor has been held annually since in Polanica Zdroj with a glittering list of top flight winners .\n",
            "Target Sentence The Rubinstein Memorial tournament in his honor has been held annually since 1963 in Polanica Zdroj , with a glittering list of top-flight winners .\n",
            "Predicted Sentence of is is is the is of . <end>\n",
            "Source Sentence In another short lived kingdom of Daliang was established by She Chongming in Chongqing as its capital .\n",
            "Target Sentence In 1621 , another short-lived kingdom of Daliang was established there .\n",
            "Predicted Sentence of is is is is is . <end>\n",
            "Source Sentence This is the same list used for the season except for Lorenzo Michelle Olga and Rebekah which replaced Luis Marilyn Opal and Roxanne .\n",
            "Target Sentence This is the same list that was used for the 1995 season except for Lorenzo , Michelle , Olga , and Rebekah .\n",
            "Predicted Sentence of is is is the is of the is and and and and of\n",
            "Source Sentence The French sometimes call basil l herbe royale .\n",
            "Target Sentence The French call basil `` herbe royale '' .\n",
            "Predicted Sentence in is is the the is the is . <end>\n",
            "Source Sentence The sabre target covers everything above the waist except the hands and the back of the head .\n",
            "Target Sentence The target area in sabre is everything from the waist up , except for the hands .\n",
            "Predicted Sentence of is is is the is the is . <end>\n",
            "Source Sentence The oldest modern human remains in Europe were discovered in the Cave With Bones in present day Romania .\n",
            "Target Sentence The oldest human remains found in Europe were found in Romania .\n",
            "Predicted Sentence of is is is the is is . <end>\n",
            "Source Sentence Though Babbage s engine was never built Lovelace s notes are important in the early history of computers .\n",
            "Target Sentence Though Babbage 's engine was never built , Lovelace 's notes are important in the early history of computers .\n",
            "Predicted Sentence of is is is the is is . <end>\n",
            "Source Sentence Eid al Adha Festival of Sacrifice or Greater Eid is an important religious holiday celebrated by Muslims worldwide to commemorate the willingness of Abraham to sacrifice his son as an act of obedience to God but instead was able to sacrifice a ram .\n",
            "Target Sentence Eid al-Adha `` Festival of Sacrifice '' or `` Greater Bairam '' is a religious festival celebrated by Muslimsall over the world .\n",
            "Predicted Sentence in is is is is is is is is the first is the first\n",
            "Source Sentence Its capital was Biltine .\n",
            "Target Sentence The capital of Biltine is Biltine .\n",
            "Predicted Sentence in is is the the is the . <end>\n",
            "Source Sentence Along with Lord Byron and Percy Bysshe Shelley Keats was one of the second generation of Romantic poets .\n",
            "Target Sentence He is often ranked as one of the five most important poets of the Romantic movement in English literature ; the other four are William Wordsworth , Samuel Taylor Coleridge , Lord Byron , and Percy Bysshe Shelley . Though Keats was the youngest of these poets , he also died before the others : he suffered from tuberculosis and died in Rome at the age of 25 .\n",
            "Predicted Sentence of is is is the is is is . <end>\n",
            "Source Sentence All in all a winner of Olympic proportions and a surefire read aloud . \n",
            "Target Sentence All in all , a winner of Olympic proportions and a surefire read-aloud . ''\n",
            "Predicted Sentence of is is is is is . <end>\n",
            "Source Sentence The apostrophe represents a glottal stop .\n",
            "Target Sentence The apostrophe sounds like a glottal stop .\n",
            "Predicted Sentence in is is the the is the is . <end>\n",
            "Source Sentence In when Norway was invaded by Nazi Germany Lie ordered all Norwegian ships to sail to Allied ports .\n",
            "Target Sentence In 1940 , when Norway was invaded by Germany , Lie ordered all Norwegian ships to sail to Allied ports .\n",
            "Predicted Sentence of is is is the is is is . <end>\n",
            "Source Sentence It is on the Bank branch of the Northern Line between Old Street and King s Cross St . Pancras stations .\n",
            "Target Sentence It is on the Bank branch of the Northern Line , between Old Street and King 's Cross St. Pancras stations .\n",
            "Predicted Sentence of is is is the is the is and . <end>\n",
            "Source Sentence It can also be called qixianqin .\n",
            "Target Sentence It can also be called qixianqin 七絃琴 .\n",
            "Predicted Sentence in is is the of the is of . <end>\n",
            "Source Sentence Osroene also known by the name of its capital city Edessa was a historic Syriac Aramean kingdom located in Mesopotamia which enjoyed semi autonomy to complete independence from the years of BC to AD .\n",
            "Target Sentence Osroene , also known by the name of its capital city , Edessa , was a historic kingdom located on the present-day border of Syria and Turkey . The kingdom was Assyrian and enjoyed semi-autonomy to complete independence from the years of 132 BCE to 244 AD .\n",
            "Predicted Sentence in is is is of . <end>\n",
            "Source Sentence In addition over copies were sold within a week in Mainland China .\n",
            "Target Sentence In addition , over 500,000 copies were sold within a week in China .\n",
            "Predicted Sentence <end>\n",
            "Source Sentence The second was to keep open the supply lines to their own armies in North Africa .\n",
            "Target Sentence The second was to keep open its own supply lines , the Axis to their own armies in North Africa and the Allies to supply the island of Malta .\n",
            "Predicted Sentence in is is the is the is is the is the the the is\n",
            "Source Sentence Some subterranean features are secret places most like road tunnels subway systems and sewers are parts of public infrastructure .\n",
            "Target Sentence Some subterranean features are secret places ; most , like road tunnels , subway systems and sewers , are parts of public infrastructure .\n",
            "Predicted Sentence of is is is is is is . <end>\n",
            "Source Sentence The Geneva Accords partitioned the country in two with a promise of democratic election to reunite the country .\n",
            "Target Sentence The Geneva Accords partitioned the country in two with a promise of democratic election to reunite the country .\n",
            "Predicted Sentence of is is is the is the is . <end>\n",
            "Source Sentence Professional wrestling is a non competitive professional sport where matches are prearranged by the promotion s writing staff and is also considered an athletic performing art containing strong elements of catch wrestling mock combat and theater .\n",
            "Target Sentence Professional wrestling is a non-competitive professional sport , where matches are made by the company 's writing staff .\n",
            "Predicted Sentence in is is is is . <end>\n",
            "Source Sentence Born in Chiayi County Taiwan in Lu was raised a Christian attending a Protestant school .\n",
            "Target Sentence Lu was born in Chiayi County , Taiwan in 1945 . He was raised a Christian and attended a Protestant school .\n",
            "Predicted Sentence <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVBZdxmj82G1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(columns=['Source', 'Target', 'Predicted'])\n",
        "for i in range(3000):\n",
        "  src,target,predicted = predict_for_report2()\n",
        "  df.loc[i]=[src,target,predicted]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw7WS41b9wi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_pickle('Transformer Batch 2 3000 Sentence pd.pkl')\n",
        "df.to_csv('Transformer Batch 2 3000 Sentence pd.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
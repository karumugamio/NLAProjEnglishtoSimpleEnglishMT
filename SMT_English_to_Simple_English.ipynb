{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMT English to Simple English.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPjZuIuPsZvbid4PTsXG5jy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karumugamio/NLAProjEnglishtoSimpleEnglishMT/blob/master/SMT_English_to_Simple_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6UQB8jSYyGt",
        "colab_type": "code",
        "outputId": "50a597af-1590-4354-dd74-feaa2029d498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Opening files from Google Drive and changing current working directory to Data folder \n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "os.chdir('/gdrive/My Drive/NLAProjectWS')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJuXBH0Xpwb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "1e2e231f-7aa1-40ae-da54-acdf18cf88e4"
      },
      "source": [
        "# if lm package import fails please update the NLTK package by !pip install nltk --upgrade to version 3.5\n",
        "!pip install nltk --upgrade"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\r\u001b[K     |▎                               | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 29.7MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 34.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 35.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 37.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 40.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 41.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 41.2MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 42.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 44.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 44.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 44.0MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 44.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 44.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 44.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 44.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 44.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 184kB 44.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 194kB 44.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 204kB 44.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 215kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 235kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 245kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 256kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 266kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 276kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 286kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 296kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 307kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 368kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 378kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 389kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 399kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 409kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 419kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 430kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 440kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 460kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 471kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 481kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 491kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 501kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 512kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 522kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 532kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 542kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 552kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 563kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 573kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 583kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 593kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 604kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 614kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 624kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 634kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 645kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 655kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 665kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 686kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 696kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 706kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 716kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 727kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 737kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 747kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 757kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 768kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 778kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 788kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 798kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 808kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 819kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 829kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 839kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 849kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 860kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 870kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 880kB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 890kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 901kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 911kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 921kB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 931kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 942kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 952kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 962kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 972kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 983kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 993kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.0MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1MB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2MB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3MB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3MB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3MB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.3MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4MB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4MB 44.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 44.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.38.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434674 sha256=7913dc72e004a7dfbd600d87e2598e1945d228a796540b81a36efebbee9cfa15\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFbWnxHCpqL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All Import Statements\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "\n",
        "from nltk.lm import MLE\n",
        "\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "from nltk.translate.ibm1 import IBMModel1\n",
        "from nltk.translate.api import AlignedSent\n",
        "\n",
        "import dill as pickle\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm8T8KPsZzUE",
        "colab_type": "code",
        "outputId": "826b7b55-880c-4011-ce2d-f2fc3e574b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "os.chdir('/gdrive/My Drive/NLAProjectWS/workspace')\n",
        "print(\"NLTK version used in this note book is \",nltk.__version__)\n",
        "os.listdir()\n",
        "NormalEngData = '/gdrive/My Drive/NLAProjectWS/Data/v1_wiki.unsimplified'\n",
        "SimpleEngData = '/gdrive/My Drive/NLAProjectWS/Data/v1_wiki.simple'\n",
        "\n",
        "SentencePairFile = 'sentence pairs.pickle'\n",
        "IBMModelFile = 'IBMModel.pickle'\n",
        "LM_EN_File = 'en_model_trigram.pickle'\n",
        "LM_SI_File = 'si_model_trigram.pickle'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK version used in this note book is  3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-7oU80RaDwt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1290da24-150a-4dcb-853c-5d4bb2001175"
      },
      "source": [
        "# uncomment only when you have trouble with nltk resources. This all download everything\n",
        "nltk.download('all')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTdRbZ6kqMEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allwords_en = open(NormalEngData).read()\n",
        "allwords_si = open(SimpleEngData).read()\n",
        "en_words = allwords_en.lower().split()\n",
        "en_words.append(\"NULL\")\n",
        "en_words = set (en_words)\n",
        "si_words = allwords_si.lower().split()\n",
        "si_words.append(\"NULL\")\n",
        "si_words = set (si_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569CQnMxobPE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "350ebb33-c5df-4839-fc82-08b214d065ed"
      },
      "source": [
        "print(\"Number of Unique English Words : \",len(en_words))\n",
        "print(\"Number of Unique Simple English Words : \",len(si_words))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique English Words :  117830\n",
            "Number of Unique Simple English Words :  107091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVZc1XTaoqK_",
        "colab_type": "text"
      },
      "source": [
        "## Create Aligned Sentence Pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTk99lfJaQvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = []\n",
        "with open(NormalEngData) as normalLines, open(SimpleEngData) as simpleLines: \n",
        "    for x, y in zip(normalLines, simpleLines):\n",
        "        x = x.strip()\n",
        "        y = y.strip()\n",
        "        #print(\"{0}\\t{1}\".format(x, y))\n",
        "        pair = (x,y)\n",
        "        pairs.append(pair)\n",
        "totalPairs = len(pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHxMNpOZo23r",
        "colab_type": "text"
      },
      "source": [
        "## Split Entire Data Corpus into Train Test partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql2t8Xb9dJIt",
        "colab_type": "code",
        "outputId": "1f8439dd-8638-4af9-895d-eb7c9fcaac38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "splitRatio = 0.7\n",
        "cutoff = round(totalPairs*splitRatio)\n",
        "print(type(pairs))\n",
        "train = pairs[0:int(cutoff)]\n",
        "test = pairs[int(cutoff)+1:int(totalPairs)-1]\n",
        "print(\"Length of Total Pairs created is {}\".format(totalPairs))\n",
        "print(\"Length of train Pairs created is {}\".format(len(train)))\n",
        "print(\"Length of test Pairs created is {}\".format(len(test)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "Length of Total Pairs created is 137362\n",
            "Length of train Pairs created is 96153\n",
            "Length of test Pairs created is 41207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyLtwhUmirYP",
        "colab_type": "text"
      },
      "source": [
        "##Creating Language Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgXPiWjxpWH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_tokenized_text = []\n",
        "si_tokenized_text = []\n",
        "for p in pairs:\n",
        "    en_tokenized_text.append(nltk.word_tokenize(p[0]))\n",
        "    si_tokenized_text.append(nltk.word_tokenize(p[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sqwK2AyqNFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Tri Gram Model for this SMT Problem Statement hence N =3 for other NGrams Please replace n with other values\n",
        "n = 3\n",
        "en_model = MLE(n)\n",
        "en_train_data, en_padded_sents = padded_everygram_pipeline(n, en_tokenized_text)\n",
        "en_model.fit(en_train_data, en_padded_sents)\n",
        "\n",
        "si_model = MLE(n)\n",
        "si_train_data, si_padded_sents = padded_everygram_pipeline(n, si_tokenized_text)\n",
        "si_model.fit(si_train_data, si_padded_sents)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg3rplqos9fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simplifying read and preprocess for creating models\n",
        "def read_sents(filename):\n",
        "    sents = []\n",
        "    c=0\n",
        "    with open(filename,'r') as fi:\n",
        "        for li in fi:\n",
        "            sents.append(li.split())\n",
        "    return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPbPpaidtKuZ",
        "colab_type": "code",
        "outputId": "6db2742e-d37e-4e3c-c46e-477d7b876b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Arrived Max Count as 96153 by 0.7 times the available corpus.\n",
        "# We are using IBM Model 1 for creating Translation Probablity\n",
        "max_count = 96153\n",
        "eng_sents_all = read_sents(NormalEngData)\n",
        "fr_sents_all = read_sents(SimpleEngData)\n",
        "eng_sents = eng_sents_all[:max_count]\n",
        "fr_sents = fr_sents_all[:max_count]\n",
        "print(\"Size of english sentences: \", len(eng_sents))\n",
        "print(\"Size of french sentences: \", len(fr_sents))\n",
        "aligned_text = []\n",
        "for i in range(len(eng_sents)):\n",
        "    al_sent = AlignedSent(fr_sents[i],eng_sents[i])\n",
        "    aligned_text.append(al_sent)\n",
        "print(\"Training IBM model 1\")\n",
        "start = time.time()\n",
        "ibm_model = IBMModel1(aligned_text,5)\n",
        "\n",
        "end = time.time()\n",
        "executionTime = end - start\n",
        "print(\"Training complete, Time taken for training model is :\",executionTime)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of english sentences:  96153\n",
            "Size of french sentences:  96153\n",
            "Training smt model\n",
            "Training complete, Time taken for training model is : 444.95045161247253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FepkGu6V6NHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Its important to use binary mode \n",
        "dbfile = open(IBMModelFile, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(ibm_model, dbfile)                      \n",
        "dbfile.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iILflz-_6phW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#si_model\n",
        "# Its important to use binary mode \n",
        "dbfile1 = open(LM_SI_File, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(si_model, dbfile1)                      \n",
        "dbfile1.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LmJiFxB6pq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#si_model\n",
        "# Its important to use binary mode \n",
        "dbfile2 = open(LM_EN_File, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(en_model, dbfile2)                      \n",
        "dbfile2.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGivG-JS7BaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Its important to use binary mode \n",
        "dbfile3 = open(SentencePairFile, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(pairs, dbfile3)                      \n",
        "dbfile3.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXVUiI1w6_ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading all Pickle Files for Creating actual SMT using HMM- Viterbi Algorithm\n",
        "# Source Tutorial for Viterbi- https://youtu.be/ECu_KQV3V30\n",
        "pickle_in = open(SentencePairFile,\"rb\")\n",
        "pairs = pickle.load(pickle_in)\n",
        "pickle_in = open(LM_EN_File,\"rb\")\n",
        "en_model = pickle.load(pickle_in)\n",
        "pickle_in = open(LM_SI_File,\"rb\")\n",
        "si_model = pickle.load(pickle_in)\n",
        "pickle_in = open(IBMModelFile,\"rb\")\n",
        "ibm_model = pickle.load(pickle_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCDJOyr6uOk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(inputSentence):\n",
        "  inputSentence = inputSentence.split()\n",
        "\n",
        "  pi = defaultdict(lambda:float(1))\n",
        "  bp = defaultdict(lambda:\"a\")\n",
        "\n",
        "  for k in range(0,len(inputSentence)):\n",
        "    for eachword in si_words:\n",
        "      for(w1,w2) in si_model.something(eachword) :\n",
        "        pi_now = pi[k-1]*si_model.score(eachword,(w1,w2)) * \"translation probablity from IBM model for kth given normal word vs each word in english\"\n",
        "        \n",
        "        if (pi[k]==1 or pi[k]<pi_now):\n",
        "          pi[k] = pi_now\n",
        "          bp[k+1] = eachword\n",
        "  return bp"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}